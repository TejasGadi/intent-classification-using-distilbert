{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SECZsc9Ol6JX"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq_2oMDrl6Jb",
        "outputId": "95d3a007-aa1e-42c9-fc86-cc607e1cd526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Found existing installation: transformers 4.51.3\n",
            "Uninstalling transformers-4.51.3:\n",
            "  Successfully uninstalled transformers-4.51.3\n",
            "Found existing installation: accelerate 1.6.0\n",
            "Uninstalling accelerate-1.6.0:\n",
            "  Successfully uninstalled accelerate-1.6.0\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
            "Installing collected packages: transformers, accelerate\n",
            "Successfully installed accelerate-1.6.0 transformers-4.51.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2yD1F59l6Jb"
      },
      "source": [
        "## Data Ingestion\n",
        "\n",
        "There are 8 unique intent labels in the dataset:\n",
        "1. Intent_Lease_Abstraction\n",
        "\n",
        "2. Intent_Comparison_LOI_Lease\n",
        "\n",
        "3. Intent_Clause_Protect\n",
        "\n",
        "4. Intent_Company_research\n",
        "\n",
        "5. Intent_Transaction_Date_navigator\n",
        "\n",
        "6. Intent_Amendment_Abstraction\n",
        "\n",
        "7. Intent_Sales_Listings_Comparison\n",
        "\n",
        "8. Intent_Lease_Listings_Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tALqHWJWM-Zf"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKgAP8Btl6Jc",
        "outputId": "9d9112f2-9db3-4180-8f08-f71240fc52f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['email_text', 'intent'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['email_text', 'intent'],\n",
            "        num_rows: 800\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load local CSV file\n",
        "raw_dataset = load_dataset(\n",
        "    'csv',\n",
        "    data_files={\n",
        "        'train': './intent_train_dataset.csv',\n",
        "        'validation': './intent_test_dataset.csv'\n",
        "    }\n",
        ")\n",
        "\n",
        "# Check the loaded dataset\n",
        "print(raw_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J17NNKKMl6Jd"
      },
      "source": [
        "## Data preprocesing\n",
        "(DistilBERT requires tokenized inputs, and Hugging Face makes this straightforward.)\n",
        "\n",
        "- Tokenization: Use the DistilBertTokenizer to convert text into token IDs.\n",
        "- Truncation and Padding: Ensure all sequences are the same length by truncating longer texts and padding shorter ones.\n",
        "- Batching: Group your data into batches for faster processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z28MB_Vxl6Jd",
        "outputId": "fd874725-6344-48db-b41c-5b833c424c0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'email_text': 'Kindly abstract the lease document (attached) for the Miller Plaza Project location. Focus on rent, term, and landlord info.', 'intent': 'Intent_Lease_Abstraction', 'input_ids': [101, 19045, 10061, 1996, 10084, 6254, 1006, 4987, 1007, 2005, 1996, 4679, 8232, 2622, 3295, 1012, 3579, 2006, 9278, 1010, 2744, 1010, 1998, 18196, 18558, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "label2id = {\n",
        "    \"Intent_Lease_Abstraction\": 0,\n",
        "    \"Intent_Comparison_LOI_Lease\": 1,\n",
        "    \"Intent_Clause_Protect\": 2,\n",
        "    \"Intent_Company_research\": 3,\n",
        "    \"Intent_Transaction_Date_navigator\": 4,\n",
        "    \"Intent_Amendment_Abstraction\": 5,\n",
        "    \"Intent_Sales_Listings_Comparison\": 6,\n",
        "    \"Intent_Lease_Listings_Comparison\": 7,\n",
        "}\n",
        "\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "# Preprocessing function with label mapping\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the text field\n",
        "    tokenized = tokenizer(examples['email_text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Convert class labels to integers\n",
        "    tokenized[\"labels\"] = [label2id[label] for label in examples[\"intent\"]]\n",
        "    return tokenized\n",
        "\n",
        "# Apply preprocessing\n",
        "encoded_dataset = raw_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Check processed data\n",
        "print(encoded_dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct6prSX8l6Jd"
      },
      "source": [
        "## Load base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zalNwQ2Ul6Je",
        "outputId": "508c755a-d2b6-41aa-a63f-8d312e953291"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "# Load DistilBERT model for classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDe1WdEZl6Je"
      },
      "source": [
        "##  Fine-Tuning DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdozu8NBl6Je"
      },
      "outputs": [],
      "source": [
        "# Training Args\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",           # Directory for saving results\n",
        "    eval_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
        "    learning_rate=5e-5,              # Initial learning rate\n",
        "    per_device_train_batch_size=16,  # Batch size per GPU\n",
        "    num_train_epochs=3,              # Number of epochs\n",
        "    weight_decay=0.01,               # Regularization\n",
        "    logging_dir=\"./logs\",            # Directory for logs\n",
        "    logging_steps=10                 # Log every 10 steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyDNlHgEl6Jf"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pwBXedsl6Jf"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "hJS7FTy7l6Jf",
        "outputId": "6fcfb152-6dff-465d-9d9e-bb550e43eb7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtvggamermax\u001b[0m (\u001b[33mtvggamermax-na\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_150418-v60o4b20</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tvggamermax-na/huggingface/runs/v60o4b20' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/tvggamermax-na/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tvggamermax-na/huggingface' target=\"_blank\">https://wandb.ai/tvggamermax-na/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tvggamermax-na/huggingface/runs/v60o4b20' target=\"_blank\">https://wandb.ai/tvggamermax-na/huggingface/runs/v60o4b20</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 18:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.089437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.091196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.090211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=0.03576590119938677, metrics={'train_runtime': 1143.1293, 'train_samples_per_second': 20.995, 'train_steps_per_second': 1.312, 'total_flos': 3179557748736000.0, 'train_loss': 0.03576590119938677, 'epoch': 3.0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Monitoring Training\n",
        "# import wandb\n",
        "# wandb.login()  # Log in to your account\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                          # The DistilBERT model\n",
        "    args=training_args,                   # Training arguments\n",
        "    train_dataset=encoded_dataset['train'],  # Training data\n",
        "    eval_dataset=encoded_dataset['validation']  # Validation data\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1c36zsZl6Jf"
      },
      "source": [
        "## Model Evaluation\n",
        "- When evaluating a fine-tuned model like DistilBERT, metrics like accuracy and F1-score are essential — but they only tell part of the story.\n",
        "\n",
        "- Take a deeper dive into classification reports and confusion matrices. These give you a clear picture of where the model excels and where it struggles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "qmvCZAZtl6Jf",
        "outputId": "69729923-59e9-4f9b-bf89-a033b46df469"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.77      0.87       100\n",
            "           1       1.00      1.00      1.00       100\n",
            "           2       0.81      1.00      0.90       100\n",
            "           3       1.00      1.00      1.00       100\n",
            "           4       1.00      1.00      1.00       100\n",
            "           5       1.00      1.00      1.00       100\n",
            "           6       0.98      1.00      0.99       100\n",
            "           7       1.00      0.98      0.99       100\n",
            "\n",
            "    accuracy                           0.97       800\n",
            "   macro avg       0.97      0.97      0.97       800\n",
            "weighted avg       0.97      0.97      0.97       800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions\n",
        "predictions = trainer.predict(encoded_dataset['validation'])\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = encoded_dataset['validation']['labels']\n",
        "\n",
        "# Generate a classification report\n",
        "print(classification_report(true_labels, predicted_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQB8rJ17l6Jf",
        "outputId": "37a9a275-100e-4afb-ceac-2acafd6b2ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 0:\n",
            "Text: Provide an abstraction for the attached lease related to Boone Light Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 1:\n",
            "Text: Provide an abstraction for the attached lease related to Johns Drives Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 3:\n",
            "Text: Provide an abstraction for the attached lease related to Reyes Summit Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 5:\n",
            "Text: Provide an abstraction for the attached lease related to Patricia Way Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 18:\n",
            "Text: Provide an abstraction for the attached lease related to Nicholas Ridge Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 25:\n",
            "Text: Provide an abstraction for the attached lease related to Reynolds Pine Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 27:\n",
            "Text: Provide an abstraction for the attached lease related to Brown Bridge Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 35:\n",
            "Text: Provide an abstraction for the attached lease related to Jacobs Shores Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 44:\n",
            "Text: Provide an abstraction for the attached lease related to Tucker Ports Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 47:\n",
            "Text: Provide an abstraction for the attached lease related to Holland Lodge Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 50:\n",
            "Text: Provide an abstraction for the attached lease related to Martinez Lights Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 54:\n",
            "Text: Provide an abstraction for the attached lease related to Victoria Roads Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 57:\n",
            "Text: Provide an abstraction for the attached lease related to Patrick Manors Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 58:\n",
            "Text: Provide an abstraction for the attached lease related to Schmitt Street Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 66:\n",
            "Text: Provide an abstraction for the attached lease related to Cooley Harbor Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 73:\n",
            "Text: Provide an abstraction for the attached lease related to John Trail Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 78:\n",
            "Text: Provide an abstraction for the attached lease related to Brenda Camp Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 81:\n",
            "Text: Provide an abstraction for the attached lease related to Clark Skyway Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 86:\n",
            "Text: Provide an abstraction for the attached lease related to Nancy Common Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 89:\n",
            "Text: Provide an abstraction for the attached lease related to Flynn Oval Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 90:\n",
            "Text: Provide an abstraction for the attached lease related to Scott Park Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 91:\n",
            "Text: Provide an abstraction for the attached lease related to Sharon Walks Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 99:\n",
            "Text: Provide an abstraction for the attached lease related to Andrews Mountains Project. Key terms and clauses needed.\n",
            "True Label: 0, Predicted Label: 2\n",
            "Example 702:\n",
            "Text: Side-by-side comparison needed for lease packages for Sandoval Islands Project.\n",
            "True Label: 7, Predicted Label: 6\n",
            "Example 751:\n",
            "Text: Side-by-side comparison needed for lease packages for Bell Mission Project.\n",
            "True Label: 7, Predicted Label: 6\n"
          ]
        }
      ],
      "source": [
        "# Error analysis\n",
        "\n",
        "# Inspect misclassified examples\n",
        "for i, (true, pred) in enumerate(zip(true_labels, predicted_labels)):\n",
        "    if true != pred:\n",
        "        print(f\"Example {i}:\")\n",
        "        print(f\"Text: {encoded_dataset['validation']['email_text'][i]}\")\n",
        "        print(f\"True Label: {true}, Predicted Label: {pred}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S94CkD_Xl6Jg",
        "outputId": "710c96a5-302c-4097-c43b-ed42ea3f668c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./finetuned_model/tokenizer_config.json',\n",
              " './finetuned_model/special_tokens_map.json',\n",
              " './finetuned_model/vocab.txt',\n",
              " './finetuned_model/added_tokens.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./finetuned_model\")\n",
        "tokenizer.save_pretrained(\"./finetuned_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "celB4gcbl6Jg"
      },
      "outputs": [],
      "source": [
        "# Load them back when needed for inference\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"./finetuned_model\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"./finetuned_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-L4QcwEl6Jg"
      },
      "outputs": [],
      "source": [
        "# # Inference\n",
        "# from transformers import pipeline\n",
        "\n",
        "# # Load pipeline\n",
        "# classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# # Sample email\n",
        "# email_text = \"Please extract the rent and renewal terms from the lease document for 45 Pine St.\"\n",
        "\n",
        "# # Run inference\n",
        "# predictions = classifier(email_text)\n",
        "\n",
        "# # Show prediction\n",
        "# print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4r3OSG2aDCw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def predict(text: str):\n",
        "    # Check if CUDA is available and set the device accordingly\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the model to the same device\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize the input text and move to the same device\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Ensure input tensors are on the same device\n",
        "\n",
        "    # Perform prediction\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits.argmax(axis=1).item()\n",
        "\n",
        "    return {\"text\": text, \"prediction\": predictions}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX1Q960PXnq5",
        "outputId": "bc6955bc-1291-434a-a309-93d47c1fd5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'Could you do a background check on Wexford Corp before we proceed? I’m particularly interested in any public disputes or bankruptcies in the past 5 years.', 'prediction': 3} \n",
            " Intent_Company_research\n"
          ]
        }
      ],
      "source": [
        "email_text = \"Could you do a background check on Wexford Corp before we proceed? I’m particularly interested in any public disputes or bankruptcies in the past 5 years.\"\n",
        "pred = predict(email_text)\n",
        "pred_class = id2label[pred[\"prediction\"]]\n",
        "print(pred, \"\\n\" ,pred_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMwl_mqTa63q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
